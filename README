
# Analysis of IDS datasets with ENN and Port Test

This project applies two quality assessment methods from the *Bad Design Smells in Benchmark NIDS Datasets* paper to the **CICIDS2017 dataset** for now and will be updated to run it on more datasets:

1. **Edited Nearest Neighbour Rule (ENN)** – detects mislabeled or noisy samples by checking whether each instance agrees with its *k* nearest neighbors.
2. **Port Test** – flags potential ambiguity in dataset labeling when benign traffic uses well-known background service ports (e.g., DNS 53, NTP 123).

---

## Results Summary

We evaluated all CSV files in CICIDS2017 using both methods.

* **ENN Misclassification Rate**:

  * Most large attack classes (DDoS, PortScan, DoS Hulk, DoS GoldenEye, Patator, etc.) had **very low misclassification rates (≈ 0%)**, suggesting that these classes are well-separated and labels are clean.
  * Some smaller attack classes (e.g., **Infiltration, Web Attacks**) showed **very high misclassification rates (50–70%)**, indicating possible label noise or overlapping traffic patterns.
  * Example:

    * *Web Attack – SQL Injection*: 71% misrate
    * *Infiltration*: 55% misrate
    * *Web Attack – XSS*: 50% misrate

* **Port Test (UGT\_C ratio)**:

  * For **attack traffic**, UGT\_C ≈ 0 across all classes. This makes sense since attacks target application ports rather than background services.
  * For **benign traffic**, UGT\_C was consistently high (≈ 0.3–0.5), showing that a significant fraction of benign flows are on background service ports such as DNS (53), NTP (123), etc.
  * Example:

    * *BENIGN in Friday DDoS*: UGT\_C = 0.34
    * *BENIGN in Monday traffic*: UGT\_C = 0.42
    * *BENIGN in Tuesday traffic*: UGT\_C = 0.46

---

## Interpretation

* **ENN** highlights *label noise and class overlap*.

  * Some small attack categories are not reliably separated, which may affect classifier training.
* **Port Test** highlights *design issues in dataset construction*.

  * Benign traffic is partially ambiguous because it heavily relies on background services, which can also be abused in real attacks.

Together, the two methods reveal **complementary dataset weaknesses**:

* Large, simple attack classes (e.g., DDoS) are clean.
* Smaller, complex attacks (e.g., SQL Injection, Infiltration) show labeling ambiguity.
* Benign traffic shows systematic ambiguity due to background service ports.

---

## Next Steps

* Extend analysis to other IDS datasets (will start with more from Konstantinous table) for comparison.
* Visualize ENN vs. Port Test results with more plots